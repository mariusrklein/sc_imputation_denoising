""" Plotting functions for evaluation of imputation methods

This module contains functions to plot the results of the evaluation of imputation methods.
The evaluation results should be supplied in a dataframe generated by 
sc_imputation_denoising.evaluation.evaluation_workflow or directly by 
sc_imputation_denoising.evaluation.evaluation_metrics.metrics

Functions:
    get_combined_score_df: Combine all metrics dataframe to get information recovery and cluster
        separation scores for each imputation method
    plot_performance_matrix: Plot a performance matrix for all evaluated imputation methods
    plot_umap_overview: Plot UMAP embeddings of the imputed datasets

Author: Marius Klein, July 2023

"""

import numpy as np
import pandas as pd
import anndata as ad
from sklearn.preprocessing import MinMaxScaler
import seaborn as sns

from sc_imputation_denoising.evaluation.utils import (
    generate_color_palette,
    generate_color_palette_2d,
)
from sc_imputation_denoising.imputation.constants import const


def get_combined_score_df(results_df):
    """Combine all metrics dataframe to get a single score for each imputation method

    :param results_df: Dataframe with columns for each metric and rows for each imputation method/dataset
        The column names should be the same as the metric names returned by
        sc_imputation_denoising.evaluation.evaluation_metrics.metrics
    """
    analysis_cols = [
        "kMeans_ari",
        "Silhouette_Score",
        "Davies_Bouldin_Score",
        "Calinski_Harabasz_Score",
        "MSE_values",
        "MSE_variance",
        "Cell_Correlation",
        "Ion_Correlation_Deviation",
    ]
    missing_cols = set(analysis_cols).difference(results_df.columns)

    if len(missing_cols) > 0:
        raise ValueError(f"Missing columns in results table: {missing_cols}")

    analysis_df = results_df[analysis_cols]

    scaler = MinMaxScaler()
    scaled_analysis_df = pd.DataFrame(
        scaler.fit_transform(analysis_df),
        columns=analysis_df.columns,
        index=analysis_df.index,
    )

    scaled_analysis_df["cluster_separation"] = (
        scaled_analysis_df["kMeans_ari"]
        + scaled_analysis_df["Silhouette_Score"]
        + scaled_analysis_df["Davies_Bouldin_Score"]
        + scaled_analysis_df["Calinski_Harabasz_Score"]
    ) / 4
    scaled_analysis_df["information_recovery"] = (
        scaled_analysis_df["Cell_Correlation"]
        + scaled_analysis_df["Ion_Correlation_Deviation"]
        + scaled_analysis_df["MSE_variance"]
        + scaled_analysis_df["MSE_values"]
    ) / 4

    scaled_analysis_df.columns = scaled_analysis_df.columns + "_scaled"
    return pd.concat([results_df, scaled_analysis_df], axis=1)


def plot_performance_matrix(
    results_df,
    imp_variable="imputation",
    size_variable=None,
    col_variable=None,
    save_path=None,
    **plot_kws,
) -> sns.FacetGrid:
    """Plot a performance matrix for all evaluated imputation methods

    :param results_df: Dataframe with columns for each metric and rows for each imputation method/dataset
        The column names should be the same as the metric names returned by
        sc_imputation_denoising.evaluation.evaluation_metrics.metrics
    :param imp_variable: str, name of the column in results_df that contains the imputation method names
    :param size_variable: str, name of the column in results_df that contains the size variable.
        Defaults to None, in which case no size variable is used
    :param **plot_kws: additional keyword arguments to pass to seaborn.relplot

    :returns: seaborn.FacetGrid plot for further customization
    """

    from adjustText import adjust_text

    if "hue_order" in plot_kws:
        imputations = plot_kws["hue_order"]
    else:
        imputations = results_df[imp_variable].unique()

    default_kws = dict(
        x="information_recovery_scaled",
        y="cluster_separation_scaled",
        hue=imp_variable,
        hue_order=imputations,
        facet_kws=dict(legend_out=True),
        height=4,
        aspect=1,
    )

    if size_variable is not None:
        default_kws["size"] = size_variable
        default_kws["sizes"] = (20, 80)

    if col_variable is not None:
        default_kws["col"] = col_variable
        default_kws["col_wrap"] = min(4, len(results_df[col_variable].unique()) - 1)

        groups = results_df[col_variable].dropna().unique()
        imp_order = results_df.sort_values([col_variable]).drop_duplicates(imp_variable)
        n_colors_na = len(imp_order[pd.isna(imp_order[col_variable])])
        n_colors = [len(imp_order[imp_order[col_variable] == c]) for c in groups] + [
            n_colors_na
        ]
        groups = list(groups) + [np.nan]
        if "palette" in plot_kws:
            cmap = generate_color_palette_2d(
                n_colors_list=n_colors, palette=plot_kws["palette"]
            )
        else:
            cmap = generate_color_palette_2d(n_colors_list=n_colors)

        cmap_dict = dict(zip(imp_order[imp_variable], cmap))
        cset = [cmap_dict[g] for g in imputations]

        ctrl_rows = results_df[pd.isna(results_df[col_variable])]
        for group in results_df[col_variable].dropna().unique():
            ctrl_rows_imp = ctrl_rows.copy()
            ctrl_rows_imp[col_variable] = group
            results_df = pd.concat([results_df, ctrl_rows_imp], axis=0)

    else:  # if no column variable given
        if "palette" in plot_kws:
            cset = generate_color_palette(len(imputations), palette=plot_kws["palette"])
        else:
            cset = generate_color_palette(len(imputations))

    default_kws["palette"] = cset

    if plot_kws is not None:
        default_kws.update(plot_kws)
    plot_kws = default_kws

    eval_plot = sns.relplot(data=results_df.reset_index(), **plot_kws)

    eval_plot.set_xlabels(r"information recovery $\Rightarrow$")
    eval_plot.set_ylabels(r"cluster separation $\Rightarrow$")

    for ax in eval_plot.axes.flat:
        ax.axhline(0.5)
        ax.axvline(0.5)
        ax.set_ylim(-0.1, 1.1)
        ax.set_xlim(-0.1, 1.1)
        ax.set_xticks([])
        ax.set_yticks([])

    handles, labels = eval_plot.axes.flat[0].get_legend_handles_labels()
    leg_colors = [handle.get_facecolor()[0] for handle in handles]
    legend = dict(zip(labels, handles))
    leg_new = {}
    leg_outer = {}
    for lkey in legend.keys():
        leg_new[lkey] = legend[lkey]

    # iterate over individual plot panels
    for (row, col, hue), data_table in list(eval_plot.facet_data()):
        for i, imputation in enumerate(imputations):
            data = data_table[data_table["imputation"] == imputation]

            if len(data) < 2:
                continue
            from scipy.interpolate import splprep, splev

            tck, u = splprep(
                [
                    data["information_recovery_scaled"],
                    data["cluster_separation_scaled"],
                ],
                s=0,
                k=1,
            )
            u_new = np.linspace(u.min(), u.max(), 100)
            x_new, y_new = splev(u_new, tck, der=0)
            line_style = "solid"
            data = data.reset_index()

            for j in range(len(data["information_recovery_scaled"]) - 1):
                eval_plot.axes.flat[col].plot(
                    [
                        data["information_recovery_scaled"][j],
                        data["information_recovery_scaled"][j + 1],
                    ],
                    [
                        data["cluster_separation_scaled"][j],
                        data["cluster_separation_scaled"][j + 1],
                    ],
                    color=cset[i],
                    linestyle=line_style,
                    linewidth=1,
                )

    eval_plot.tight_layout()

    if save_path is not None:
        eval_plot.fig.savefig(save_path)
    return eval_plot


def plot_umap_overview(umap_df, save_path=None, **plot_kws) -> sns.FacetGrid:
    """Plot UMAP embeddings of the imputed datasets

    :param umap_df: Dataframe with UMAP embeddings for each imputed dataset. Should contain columns
        for UMAP coordinates of all cells ('X_umap-0', 'X_umap-1'), dataset metadata like the cell
        conditions (assuming const.CONDITION_COL) and imputation metadata like the dropout rate
        (assuming "dropout_rate") and imputation method (assuming "imputation").
    :param save_path: str, path to save the plot to. Defaults to None, in which case the plot is not saved

    returns: seaborn.FacetGrid plot for further customization
    """
    n_conditions = umap_df[const.CONDITION_COL].nunique()

    default_kws = dict(
        x="X_umap-0",
        y="X_umap-1",
        s=10,
        linewidth=0,
        col="imputation",
        hue=const.CONDITION_COL,
        facet_kws=dict(
            margin_titles=True, sharex=False, sharey=False, legend_out=False
        ),
        height=4,
    )

    if 'dropout_rate' in umap_df.columns:
        default_kws['row'] = "dropout_rate"
    else:
        print("No 'dropout_rate' column found, assuming all data matrices have the same dropout " +
              "rate. Facetting only by imputation.")
        default_kws['col_wrap'] = min(4, n_conditions)

    if plot_kws is not None:
        default_kws.update(plot_kws)
    plot_kws = default_kws

    plot = sns.relplot(data=umap_df, **plot_kws)

    plot.set_xlabels("UMAP 1")
    plot.set_ylabels("UMAP 2")
    for ax in plot.axes.flat:
        ax.set_xticks([])
        ax.set_yticks([])

    if 'dropout_rate' in umap_df.columns and plot_kws["row"] == "dropout_rate":
        plot_kws["col"] = "DR"

        plot.set_titles(
            row_template=f"{plot_kws['row']} = " + "{row_name}",
            col_template=f"{plot_kws['col']} = " + "{col_name}",
        )

    plot.fig.tight_layout()
    sns.move_legend(
        plot, "lower center", bbox_to_anchor=(0.5, 1), ncol=n_conditions, frameon=False
    )
    if save_path is not None:
        plot.fig.savefig(save_path)

    return plot
